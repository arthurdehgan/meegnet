{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69de7617-055c-4d98-9a2b-feeeef194c90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3850420894.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    input_tensor = # Create an input tensor image for your model..\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "target_layers = [model.layer4[-1]]\n",
    "input_tensor = # Create an input tensor image for your model..\n",
    "# Note: input_tensor can be a batch tensor with several images!\n",
    "\n",
    "# Construct the CAM object once, and then re-use it on many images:\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "\n",
    "# You can also use it within a with statement, to make sure it is freed,\n",
    "# In case you need to re-create it inside an outer loop:\n",
    "# with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "#   ...\n",
    "\n",
    "# We have to specify the target we want to generate\n",
    "# the Class Activation Maps for.\n",
    "# If targets is None, the highest scoring category\n",
    "# will be used for every image in the batch.\n",
    "# Here we use ClassifierOutputTarget, but you can define your own custom targets\n",
    "# That are, for example, combinations of categories, or specific outputs in a non standard model.\n",
    "\n",
    "targets = [ClassifierOutputTarget(281)]\n",
    "\n",
    "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "# In this example grayscale_cam has only one image in the batch:\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "# You can also get the model outputs without having to re-inference\n",
    "model_outputs = cam.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ca44f1-cb45-43ad-8a18-89413bbdd938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from meegnet.parsing import parser\n",
    "from meegnet.params import TIME_TRIAL_LENGTH\n",
    "from meegnet.utils import load_checkpoint, cuda_check\n",
    "from meegnet.network import create_net\n",
    "from meegnet.dataloaders import load_data\n",
    "from meegnet.util_viz import (\n",
    "    get_positive_negative_saliency,\n",
    "    compute_saliency_based_psd,\n",
    ")\n",
    "from pytorch_grad_cam import GuidedBackpropReLUModel\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "DEVICE = cuda_check()\n",
    "CHANNELS = (\"MAG\", \"PLANAR1\", \"PLANAR2\")\n",
    "\n",
    "def compute_saliency_maps(sub, sal_path, GBP, args):\n",
    "    data, targets = load_data(\n",
    "        dataframe.loc[dataframe[\"sub\"] == sub],\n",
    "        args.data_path,\n",
    "        epoched=args.epoched,\n",
    "        seed=args.seed,\n",
    "        s_freq=args.sfreq,\n",
    "        chan_index=chan_index,\n",
    "        datatype=args.datatype,\n",
    "        eventclf=args.eventclf,\n",
    "    )\n",
    "    if data is None or targets is None:\n",
    "        return\n",
    "    if args.eventclf:\n",
    "        target_saliencies = [[[], []], [[], []]]\n",
    "        target_psd = [[[], []], [[], []]]\n",
    "    else:\n",
    "        target_saliencies = [[], []]\n",
    "        target_psd = [[], []]\n",
    "\n",
    "    # For each of those trial with associated label:\n",
    "    for trial, label in zip(data, targets):\n",
    "        X = trial[np.newaxis].type(torch.FloatTensor).to(DEVICE)\n",
    "        if len(X.shape) < 4:\n",
    "            X = X[np.newaxis, :]\n",
    "        # Compute predictions of the trained network, and confidence\n",
    "        preds = torch.nn.Softmax(dim=1)(net(X)).detach().cpu()\n",
    "        pred = preds.argmax().item()\n",
    "        confidence = preds.max()\n",
    "        if args.subclf:\n",
    "            label = int(dataframe[dataframe[\"sub\"] == sub].index[0])\n",
    "\n",
    "        # If the confidence reaches desired treshhold (given by args.confidence)\n",
    "        if confidence >= args.confidence and pred == label:\n",
    "            # Compute Guided Back-propagation for given label projected on given data X\n",
    "            guided_grads = GBP(X.to(DEVICE), label)\n",
    "            guided_grads = np.rollaxis(guided_grads, 2, 0)\n",
    "            # Compute saliencies\n",
    "            pos_saliency, neg_saliency = get_positive_negative_saliency(guided_grads)\n",
    "\n",
    "            # Depending on the task, add saliencies in lists\n",
    "            if args.eventclf:\n",
    "                target_saliencies[label][0].append(pos_saliency)\n",
    "                target_saliencies[label][1].append(neg_saliency)\n",
    "                if args.compute_psd:\n",
    "                    target_psd[label][0].append(\n",
    "                        compute_saliency_based_psd(\n",
    "                            pos_saliency, trial, args.w_size, args.sfreq\n",
    "                        )\n",
    "                    )\n",
    "                    target_psd[label][1].append(\n",
    "                        compute_saliency_based_psd(\n",
    "                            neg_saliency, trial, args.w_size, args.sfreq\n",
    "                        )\n",
    "                    )\n",
    "            else:\n",
    "                target_saliencies[0].append(pos_saliency)\n",
    "                target_saliencies[1].append(neg_saliency)\n",
    "                if args.compute_psd:\n",
    "                    target_psd[0].append(\n",
    "                        compute_saliency_based_psd(\n",
    "                            pos_saliency, trial, args.w_size, args.sfreq\n",
    "                        )\n",
    "                    )\n",
    "                    target_psd[1].append(\n",
    "                        compute_saliency_based_psd(\n",
    "                            neg_saliency, trial, args.w_size, args.sfreq\n",
    "                        )\n",
    "                    )\n",
    "    # With all saliencies computed, we save them in the specified save-path\n",
    "    for j, sal_type in enumerate((\"pos\", \"neg\")):\n",
    "        if args.eventclf:\n",
    "            for i, label in enumerate(labels):\n",
    "                sal_filepath = os.path.join(\n",
    "                    sal_path,\n",
    "                    f\"{sub}_{labels[i]}_{sal_type}_sal_{args.confidence}confidence.npy\",\n",
    "                )\n",
    "                np.save(sal_filepath, np.array(target_saliencies[i][j]))\n",
    "                if args.compute_psd:\n",
    "                    psd_filepath = os.path.join(\n",
    "                        psd_path,\n",
    "                        f\"{sub}_{labels[i]}_{sal_type}_psd_{args.confidence}confidence.npy\",\n",
    "                    )\n",
    "                    np.save(psd_filepath, np.array(target_psd[i][j]))\n",
    "        else:\n",
    "            lab = \"\" if args.subclf else f\"_{labels[label]}\"\n",
    "            sal_filepath = os.path.join(\n",
    "                sal_path,\n",
    "                f\"{sub}{lab}_{sal_type}_sal_{args.confidence}confidence.npy\",\n",
    "            )\n",
    "            np.save(sal_filepath, np.array(target_saliencies[j]))\n",
    "            if args.compute_psd:\n",
    "                lab = \"\" if args.subclf else f\"_{labels[label]}\"\n",
    "                psd_filepath = os.path.join(\n",
    "                    psd_path,\n",
    "                    f\"{sub}{lab}_{sal_type}_psd_{args.confidence}confidence.npy\",\n",
    "                )\n",
    "                np.save(psd_filepath, np.array(target_psd[j]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e91fadf3-b57d-42d0-8909-a10c116f2e9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#####################################\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m### LOADING NETWORK AND DATA INFO ###\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#####################################\u001b[39;00m\n\u001b[1;32m     24\u001b[0m model_filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MODEL_PATH, name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m net \u001b[38;5;241m=\u001b[39m create_net(net_option, name, input_size, n_outputs, DEVICE, \u001b[43margs\u001b[49m)\n\u001b[1;32m     26\u001b[0m _, net_state, _ \u001b[38;5;241m=\u001b[39m load_checkpoint(model_filepath)\n\u001b[1;32m     27\u001b[0m net\u001b[38;5;241m.\u001b[39mload_state_dict(net_state)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "labels = [\"visual\", \"auditory\"]  # image is label 0 and sound label 1\n",
    "trial_length = TIME_TRIAL_LENGTH\n",
    "n_channels = 306\n",
    "chan_index = [0, 1, 2]\n",
    "input_size = (n_channels // 102, 102, trial_length)\n",
    "fold = 1\n",
    "name = f\"net_42_fold{fold}_ALL\"\n",
    "suffixes = \"\"\n",
    "n_outputs = 2\n",
    "epoched = True\n",
    "datatype = \"passive\"\n",
    "seed=43\n",
    "max_subj=300\n",
    "net_option=\"best_net\"\n",
    "\n",
    "MODEL_PATH = \"/home/arthur/github/meegnet/\"\n",
    "DATA_PATH = \"/home/arthur/github/meegnet/data/\"\n",
    "\n",
    "\n",
    "#####################################\n",
    "### LOADING NETWORK AND DATA INFO ###\n",
    "#####################################\n",
    "\n",
    "model_filepath = os.path.join(MODEL_PATH, name + \".pt\")\n",
    "net = create_net(net_option, name, input_size, n_outputs, DEVICE, args)\n",
    "_, net_state, _ = load_checkpoint(model_filepath)\n",
    "net.load_state_dict(net_state)\n",
    "\n",
    "dataframe = (\n",
    "    pd.read_csv(\n",
    "        os.path.join(DATA_PATH, f\"participants_info_{datatype}.csv\"),\n",
    "        index_col=0,\n",
    "    )\n",
    "    .sample(frac=1, random_state=seed)\n",
    "    .reset_index(drop=True)[:max_subj]\n",
    ")\n",
    "subj_list = dataframe[\"sub\"]\n",
    "\n",
    "#################\n",
    "### MAIN LOOP ###\n",
    "#################\n",
    "\n",
    "target_layers = [net.layer4[-1]]\n",
    "data, targets = load_data(\n",
    "        dataframe.loc[dataframe[\"sub\"] == sub],\n",
    "        DATA_PATH,\n",
    "        epoched=epoched,\n",
    "        seed=seed,\n",
    "        s_freq=500,\n",
    "        chan_index=chan_index,\n",
    "        datatype=datatype,\n",
    "        eventclf=eventclf,\n",
    "    )\n",
    "trial, label = next(zip(data, targets))\n",
    "X = trial[np.newaxis].type(torch.FloatTensor).to(DEVICE)\n",
    "if len(X.shape) < 4:\n",
    "    X = X[np.newaxis, :]\n",
    "\n",
    "input_tensor = X # Create an input tensor image for your model..\n",
    "# Note: input_tensor can be a batch tensor with several images!\n",
    "\n",
    "# Construct the CAM object once, and then re-use it on many images:\n",
    "cam = GradCAM(model=net, target_layers=target_layers, device=DEVICE)\n",
    "\n",
    "# You can also use it within a with statement, to make sure it is freed,\n",
    "# In case you need to re-create it inside an outer loop:\n",
    "# with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "#   ...\n",
    "\n",
    "# We have to specify the target we want to generate\n",
    "# the Class Activation Maps for.\n",
    "# If targets is None, the highest scoring category\n",
    "# will be used for every image in the batch.\n",
    "# Here we use ClassifierOutputTarget, but you can define your own custom targets\n",
    "# That are, for example, combinations of categories, or specific outputs in a non standard model.\n",
    "\n",
    "targets = [label]\n",
    "\n",
    "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "# In this example grayscale_cam has only one image in the batch:\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "# You can also get the model outputs without having to re-inference\n",
    "model_outputs = cam.outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
