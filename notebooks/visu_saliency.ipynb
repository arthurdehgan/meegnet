{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182379c5-1713-455e-b6fe-83ce0a8590b6",
   "metadata": {},
   "source": [
    "# Saliency maps tutorial\n",
    "\n",
    "In order to generate saliency maps, you need the dataset in the proper format (see the [prepare data tutorial](https://meegnet.readthedocs.io/en/latest/preparedata.html), and an architecture, trained or not (seeing saliency maps with an untrained architecture should be noise).\n",
    "\n",
    "The first step to generating visualisation for the saliency maps will be to first compute the saliency maps.\n",
    "\n",
    "## Generate Saliency maps\n",
    "\n",
    "First we will set all the needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18eb752b-3918-4b58-9cb1-56b9fde74e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from meegnet.parsing import parser, save_config\n",
    "from meegnet.dataloaders import EpochedDataset\n",
    "from meegnet.network import Model\n",
    "import meegnet.viz as mv\n",
    "from meegnet_functions import load_single_subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a5f326-3d42-418f-8a73-f205dbc46eb5",
   "metadata": {},
   "source": [
    "This next section sets up all the parameters we will need for the saliency maps computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23bba896-da3a-47e0-a9d7-497439fd49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set up our data to be 3 channel types (MAG GRAD GRAD),\n",
    "# 102 sensor locations (Elekta Neuromag Vector View 306 channel MEG),\n",
    "# and 400 time samples for 800ms of signal sampled as 500Hz.\n",
    "n_channels = \"ALL\"\n",
    "input_size = (3, 102, 400)\n",
    "\n",
    "n_outputs = 2 # using auditory vs rest stimulus classification -> 2 classes\n",
    "n_subjects = 1000 # For this tutorial we will only use a fraction of the data\n",
    "n_samples = None # We will use all trials for each subject\n",
    "\n",
    "# Setting up paths\n",
    "classif = \"eventclf\" # also used for naming files and for model name\n",
    "save_path = f\"/home/arthur/data/camcan/{classif}\"\n",
    "model_path = save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cd479-9042-4fca-ac21-36ef4463f88b",
   "metadata": {},
   "source": [
    "### Loading the Model\n",
    "\n",
    "Loading the model from pretrained, using the from_pretrained method, can also load another model using the load method. It is also possible to comment both lines in order to use an untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2cf9d96-8563-4075-a0cf-b7964fb0dbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while loading checkpoint from /home/arthur/data/camcan/eventclf/eventclf_meegnet_42_ALL.pt\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'net_state' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# my_model.from_pretrained()\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyvenvs/camcan/lib/python3.12/site-packages/meegnet/network.py:967\u001b[0m, in \u001b[0;36mModel.load\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load model from file.\"\"\"\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 967\u001b[0m     net_state, optimizer_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m net_state[\u001b[38;5;28mlist\u001b[39m(net_state\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs:\n\u001b[1;32m    969\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mload_state_dict(net_state)\n",
      "File \u001b[0;32m~/.pyvenvs/camcan/lib/python3.12/site-packages/meegnet/network.py:962\u001b[0m, in \u001b[0;36mModel._load_net\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     LOG\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while loading checkpoint from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 962\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnet_state\u001b[49m, optimizer_state\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'net_state' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import meegnet.network as mn\n",
    "import importlib\n",
    "importlib.reload(mn)\n",
    "\n",
    "# setting up a seed for reproducibility (will be used for numpy, pandas, torch, and the meegnet library)\n",
    "seed = 42 \n",
    "\n",
    "# net option can be \"meegnet\", \"eegnet\" etc, see documentation\n",
    "net_option = \"meegnet\" \n",
    "\n",
    "# name of the model\n",
    "name = f\"{classif}_{net_option}_{seed}_{n_channels}\"\n",
    "\n",
    "my_model = mn.Model(name, net_option, input_size, n_outputs, save_path=save_path)\n",
    "# my_model.from_pretrained()\n",
    "model_path = os.path.join(save_path, name + \".pt\")\n",
    "my_model.load(model_path)\n",
    "print(\"Model Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51524b3-e929-4baa-a768-7726432bc708",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "If the data was set-up correctly, we use participands_info.csv in order to generate a subject list and select a random subject for generating figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41183957-7172-48e3-9f73-4eacb52e6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(save_path, f\"participants_info.csv\")\n",
    "dataframe = (\n",
    "    pd.read_csv(csv_file, index_col=0)\n",
    "    .sample(frac=1, random_state=seed)\n",
    "    .reset_index(drop=True)[: n_subjects]\n",
    ")\n",
    "subj_list = dataframe[\"sub\"]\n",
    "np.random.seed(seed)\n",
    "random_subject_idx = np.random.choice(np.arange(len(subj_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b881d-c292-4644-93a0-b0e8f4b5738b",
   "metadata": {},
   "source": [
    "### Compute Saliency maps\n",
    "\n",
    "Finally, we compute the saliency maps and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69178e51-7074-4f0b-a32f-3f1caf9d24e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# name for the labels, will be useful for saving and generating figures\n",
    "labels = [\"stimulus\", \"rest\"]\n",
    "\n",
    "# This will create a saliency maps path inside the save path\n",
    "# Please don't change or it might break code later on\n",
    "sal_path = os.path.join(save_path, \"saliency_maps\", name)\n",
    "if not os.path.exists(sal_path):\n",
    "    os.makedirs(sal_path)\n",
    "    \n",
    "# Only keep trials with 85% prediction confidence or more\n",
    "confidence = .85\n",
    "\n",
    "for sub in subj_list:\n",
    "    # The next two lines allow to pick-up where we left off and not compute saliencies each time this cell is ran.\n",
    "    sub_files = [a for a in os.listdir(sal_path) if sub in a]\n",
    "    if len(sub_files) != n_outputs * 2: # 2 file (positive and negative saliencies) per label.\n",
    "        dataset = EpochedDataset(\n",
    "                sfreq=500, # sampling frequency of 500Hz\n",
    "                n_subjects=n_subjects,\n",
    "                n_samples=n_samples,\n",
    "                sensortype='ALL', # we use MAG GRAD GRAD here\n",
    "                lso=True,\n",
    "                random_state=seed,\n",
    "        )\n",
    "        dataset.load(save_path, one_sub=sub)\n",
    "        compute_saliency_maps(\n",
    "            dataset,\n",
    "            labels,\n",
    "            sub,\n",
    "            sal_path,\n",
    "            my_model.net,\n",
    "            threshold = confidence,\n",
    "            epoched=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488bdbea-19a2-41d4-b5c0-b4cb4b350a88",
   "metadata": {},
   "source": [
    "### Generating figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402f715-4db1-4576-be7c-49822faf31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = [\"MAG\", \"PLANAR1\", \"PLANAR2\"]\n",
    "saliency_types = (\"pos\", \"neg\")\n",
    "cmap = \"coolwarm\"\n",
    "\n",
    "info = np.load(\"../camcan_sensor_locations.npy\", allow_pickle=True).tolist()\n",
    "\n",
    "# Some tested aletrnatives for the colormap:\n",
    "# cmap = sns.color_palette(\"icefire\", as_cmap=True)\n",
    "# cmap = sns.color_palette(\"coolwarm\", as_cmap=True, center=\"dark\")\n",
    "# cmap = \"inferno\"\n",
    "# cmap = \"seismic\"\n",
    "\n",
    "def get_saliency_data(saliency_dict):\n",
    "    saliencies = {}\n",
    "    operation = lambda a, b: a - b\n",
    "    for lab, pos in saliency_dict[\"pos\"].items():\n",
    "        saliencies[lab] = operation(np.array(pos), np.array(saliency_dict[\"neg\"][lab]))\n",
    "    return saliencies\n",
    "\n",
    "all_saliencies = defaultdict(lambda: defaultdict(lambda: []))\n",
    "\n",
    "print(f\"Generating figure for sensors: {sensors}\")\n",
    "print(f\"For the visual vs resting state classification\")\n",
    "\n",
    "# First load all computed saliencies\n",
    "for i, sub in enumerate(subj_list):\n",
    "    sub_saliencies = defaultdict(lambda: {})\n",
    "    for label in labels:\n",
    "        nofile = False\n",
    "        for saliency_type in saliency_types:\n",
    "            lab = f\"_{label}\"\n",
    "            saliency_file = os.path.join(\n",
    "                sal_path,\n",
    "                f\"{sub}{lab}_{saliency_type}_sal_{confidence}confidence.npy\",\n",
    "            )\n",
    "            if os.path.exists(saliency_file):\n",
    "                try:\n",
    "                    saliencies = np.load(saliency_file)\n",
    "                    sub_saliencies[saliency_type][label] = saliencies\n",
    "                except IOError:\n",
    "                    logging.warning(f\"Error loading {saliency_file}\")\n",
    "                    nofile = True\n",
    "                    continue\n",
    "            else:\n",
    "                nofile = True\n",
    "                continue\n",
    "            if len(saliencies.shape) == 3:\n",
    "                saliencies = saliencies[np.newaxis, ...]  # If only one saliency in file\n",
    "            elif len(saliencies.shape) != 4:\n",
    "                nofile = True\n",
    "                continue\n",
    "            all_saliencies[saliency_type][label].append(saliencies.mean(axis=0))\n",
    "\n",
    "        if nofile:\n",
    "            continue\n",
    "\n",
    "    skip = False\n",
    "    if i == random_subject_idx:\n",
    "        data_dict = get_saliency_data(sub_saliencies)\n",
    "        for val in data_dict.values():\n",
    "            if val.size == 0:\n",
    "                skip = True\n",
    "                break\n",
    "        temp = {\n",
    "            key: val[np.random.choice(np.arange(len(val)))]\n",
    "            for key, val in data_dict.items()\n",
    "        }\n",
    "        out_path = mv.generate_saliency_figure(\n",
    "            temp,\n",
    "            info=info,\n",
    "            save_path=save_path,\n",
    "            suffix=f\"{classif}_{sub}_single_trial\",\n",
    "            sensors=sensors,\n",
    "            title=f\"Saliencies for a single trial of subject {sub}\",\n",
    "            cmap=cmap,\n",
    "            show=True,\n",
    "            edge=50,\n",
    "            topomap=\"average\",\n",
    "        )\n",
    "        print(f\"Figure generated: {out_path}\")\n",
    "        temp = {key: np.mean(val, axis=0) for key, val in data_dict.items()}\n",
    "        out_path = mv.generate_saliency_figure(\n",
    "            temp,\n",
    "            info=info,\n",
    "            save_path=save_path,\n",
    "            suffix=f\"{classif}_{sub}_all_trials\",\n",
    "            sensors=sensors,\n",
    "            title=f\"Saliencies for the averaged trials of subject {sub}\",\n",
    "            cmap=cmap,\n",
    "            show=True,\n",
    "            edge=50,\n",
    "            topomap=\"average\",\n",
    "        )\n",
    "        print(f\"Figure generated: {out_path}\")\n",
    "    if skip:\n",
    "        random_subject_idx += 1\n",
    "        continue\n",
    "\n",
    "for label in labels:\n",
    "    for saliency_type in saliency_types:\n",
    "        if type(all_saliencies[saliency_type][label]) == list:\n",
    "            all_saliencies[saliency_type][label] = np.array(\n",
    "                all_saliencies[saliency_type][label]\n",
    "            )\n",
    "            \n",
    "data_dict = get_saliency_data(all_saliencies)\n",
    "final_dict = {key: np.mean(val, axis=0)[np.newaxis] for key, val in data_dict.items()}\n",
    "\n",
    "out_path = mv.generate_saliency_figure(\n",
    "    final_dict,\n",
    "    info=info,\n",
    "    save_path=save_path,\n",
    "    suffix=f\"{classif}\",\n",
    "    sensors=sensors,\n",
    "    title=f\"Saliencies averaged across all subjects\",\n",
    "    cmap=cmap,\n",
    "    show=True,\n",
    "    edge=50,\n",
    "    topomap=\"average\",\n",
    ")\n",
    "print(f\"Figure generated: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d5deb9-b84c-42fb-acfe-3f380f83aca7",
   "metadata": {},
   "source": [
    "In the case of epoched data with fixed timing, it might be interresting to look at the saliency at specific timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a79774-d4ab-4074-bd7b-ba76bba1037c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stim_tick = 150 # The timing for the stimulus is 150ms\n",
    "\n",
    "# Choose a random subject:\n",
    "random_sub_id = 42\n",
    "sub = subj_list[random_sub_id]\n",
    "i = random_sub_id\n",
    "\n",
    "\n",
    "print(f\"Generating figure for sensors: {sensors}\")\n",
    "print(f\"For the visual vs resting state classification\")\n",
    "\n",
    "# Loading data and generating figure:\n",
    "sub_saliencies = defaultdict(lambda: {})\n",
    "for label in labels:\n",
    "    nofile = False\n",
    "    for saliency_type in saliency_types:\n",
    "        lab = f\"_{label}\"\n",
    "        saliency_file = os.path.join(\n",
    "            sal_path,\n",
    "            f\"{sub}{lab}_{saliency_type}_sal_{confidence}confidence.npy\",\n",
    "        )\n",
    "        if os.path.exists(saliency_file):\n",
    "            try:\n",
    "                saliencies = np.load(saliency_file)\n",
    "                sub_saliencies[saliency_type][label] = saliencies\n",
    "            except IOError:\n",
    "                logging.warning(f\"Error loading {saliency_file}\")\n",
    "                nofile = True\n",
    "                continue\n",
    "        else:\n",
    "            nofile = True\n",
    "            continue\n",
    "        if len(saliencies.shape) == 3:\n",
    "            saliencies = saliencies[np.newaxis, ...]  # If only one saliency in file\n",
    "        elif len(saliencies.shape) != 4:\n",
    "            nofile = True\n",
    "            continue\n",
    "    if nofile:\n",
    "        continue\n",
    "\n",
    "data_dict = get_saliency_data(sub_saliencies)\n",
    "for val in data_dict.values():\n",
    "    if val.size == 0:\n",
    "        skip = True\n",
    "        break\n",
    "temp = {key: np.mean(val, axis=0) for key, val in data_dict.items()}\n",
    "out_path = mv.generate_saliency_figure(\n",
    "    temp,\n",
    "    info=info,\n",
    "    save_path=save_path,\n",
    "    suffix=f\"{classif}_{sub}_all_trials_timing\",\n",
    "    sensors=sensors,\n",
    "    title=f\"Saliencies for the averaged trials of subject {sub}\",\n",
    "    cmap=cmap,\n",
    "    show=True,\n",
    "    topomap=\"window\",\n",
    "    edge=50,\n",
    "    stim_tick=stim_tick,\n",
    ");\n",
    "print(f\"Figure generated: {out_path}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c07f1a-d30c-4747-9836-dbb7edfa54c1",
   "metadata": {},
   "source": [
    "The green dashed line is the timing of thew highest saliency in the trial, and the topomap corresponds to the timing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camcan",
   "language": "python",
   "name": "camcan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
